"""
FastAPI router for chat streaming endpoints.

Provides SSE (Server-Sent Events) streaming for real-time chat responses.
"""

import asyncio
import logging
from datetime import datetime
from typing import Annotated
from fastapi import APIRouter, Depends, HTTPException, Response
from sqlalchemy.orm import Session
from sse_starlette.sse import EventSourceResponse
from app.db.session import get_db
from app.services.conversation_service import ConversationService
from app.services.message_service import MessageService
from app.services.llm_service import llm_service
from app.services.stream_manager import stream_manager
from app.schemas.message import (
    ChatStreamRequest,
    MessageCreate,
    SSETokenEvent,
    SSECompleteEvent,
    SSEErrorEvent
)

logger = logging.getLogger(__name__)

# Create router instance
router = APIRouter()


@router.post("/stream")
async def stream_chat(
    request: ChatStreamRequest,
    db: Annotated[Session, Depends(get_db)]
):
    """
    Stream LLM chat response using SSE.

    Args:
        request: Chat stream request with conversation_id and message
        db: Database session (injected)

    Returns:
        EventSourceResponse with SSE stream

    Raises:
        HTTPException 400: If validation fails
        HTTPException 404: If conversation not found
        HTTPException 503: If LLM service unavailable

    Example:
        POST /api/chat/stream
        {
            "conversation_id": 1,
            "message": "What is IEC 62443?"
        }

        Response 200 (SSE stream):
        Headers:
            Content-Type: text/event-stream
            Cache-Control: no-cache
            Connection: keep-alive
            X-Session-ID: 550e8400-e29b-41d4-a716-446655440000

        Events:
            event: token
            data: {"token": "IEC", "message_id": 2, "session_id": "550e..."}

            event: token
            data: {"token": " 62443", "message_id": 2, "session_id": "550e..."}

            event: complete
            data: {"message_id": 2, "token_count": 150, "completion_time_ms": 3500}
    """
    # Verify conversation exists
    # WHY check first: Fail fast if conversation doesn't exist.
    # Better to return 404 immediately than start streaming and fail later.
    conversation = ConversationService.get_conversation_by_id(db, request.conversation_id)
    if not conversation:
        raise HTTPException(status_code=404, detail="Conversation not found")

    # Create user message in database
    # WHY create first: We need the message ID to include in stream events.
    # This also ensures we have a database record even if streaming fails.
    user_message = MessageService.create_message(
        db,
        MessageCreate(
            conversation_id=request.conversation_id,
            role="user",
            content=request.message,
            parent_message_id=None
        )
    )

    # Create assistant message placeholder
    # WHY placeholder: We need the message ID before streaming starts.
    # The content will be empty initially and updated as tokens arrive.
    assistant_message = MessageService.create_message(
        db,
        MessageCreate(
            conversation_id=request.conversation_id,
            role="assistant",
            content="",  # Will be filled during streaming
            parent_message_id=user_message.id
        )
    )

    # Define the streaming generator
    # WHY async generator: Yields SSE events as they are generated by the LLM.
    # This is the core of the SSE streaming implementation.
    async def event_generator():
        """
        Generate SSE events from LLM stream.

        Yields SSE-formatted events (token, complete, error).
        Handles cleanup and error recovery.
        """
        session_id = None
        start_time = datetime.utcnow()
        accumulated_tokens = []
        token_count = 0

        try:
            # Get conversation history for LLM context
            # WHY context: LLM needs previous messages to generate coherent responses.
            # We limit to last 10 messages to avoid exceeding context window.
            history = MessageService.get_conversation_history(
                db, request.conversation_id, max_messages=10
            )

            # Add current user message to history
            history.append({"role": "user", "content": request.message})

            # Build prompt from history
            # WHY build_chat_prompt: Formats messages into LLM's expected format.
            # Different models use different chat templates.
            prompt = llm_service.build_chat_prompt(history)

            # Create streaming task
            # WHY task: Allows us to track and cancel the stream if needed.
            async def stream_task():
                """Inner task that generates tokens."""
                async for token in llm_service.generate_stream(
                    prompt=prompt,
                    max_tokens=2048,
                    temperature=0.7,
                    stop_sequences=["User:", "Assistant:"]
                ):
                    yield token

            # Wrap in task for cancellation support
            task = asyncio.create_task(stream_task().__anext__())

            # Register session for cancellation
            session_id = await stream_manager.create_session(task)

            # Yield session ID in initial comment
            # WHY comment: Allows us to send session_id before first event.
            # SSE clients will ignore comments (lines starting with :).
            yield {
                "comment": f"session_id:{session_id}"
            }

            # Stream tokens
            async for token in llm_service.generate_stream(
                prompt=prompt,
                max_tokens=2048,
                temperature=0.7,
                stop_sequences=["User:", "Assistant:"]
            ):
                # Accumulate token
                accumulated_tokens.append(token)
                token_count += 1

                # Yield token event
                # WHY event: token: Allows client to distinguish event types.
                # data: Contains JSON payload with token and metadata.
                event_data = SSETokenEvent(
                    token=token,
                    message_id=assistant_message.id,
                    session_id=session_id
                )
                yield {
                    "event": "token",
                    "data": event_data.model_dump_json()
                }

            # Calculate completion metrics
            completion_time_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)

            # Update assistant message with complete content
            # WHY update: We created a placeholder earlier; now we fill it.
            complete_content = "".join(accumulated_tokens)
            assistant_message.content = complete_content

            # Update message metadata
            MessageService.update_message_metadata(
                db,
                assistant_message.id,
                token_count=token_count,
                model_name="gpt-oss-20b",  # From config in future
                completion_time_ms=completion_time_ms
            )

            # Yield completion event
            complete_data = SSECompleteEvent(
                message_id=assistant_message.id,
                token_count=token_count,
                completion_time_ms=completion_time_ms
            )
            yield {
                "event": "complete",
                "data": complete_data.model_dump_json()
            }

        except asyncio.CancelledError:
            # Stream was cancelled by user
            logger.info(f"Stream cancelled: session_id={session_id}")
            error_data = SSEErrorEvent(
                error="Stream cancelled by user",
                error_type="cancelled"
            )
            yield {
                "event": "error",
                "data": error_data.model_dump_json()
            }

        except Exception as e:
            # Stream failed due to error
            logger.error(f"Stream failed: {e}")
            error_data = SSEErrorEvent(
                error=str(e),
                error_type="service_error"
            )
            yield {
                "event": "error",
                "data": error_data.model_dump_json()
            }

        finally:
            # Cleanup session
            # WHY finally: Ensures cleanup even if exception occurs.
            # Prevents memory leaks from abandoned sessions.
            if session_id:
                await stream_manager.cleanup_session(session_id)

    # Return EventSourceResponse
    # WHY EventSourceResponse: This is sse-starlette's helper that handles
    # SSE formatting, headers, and keep-alive pings automatically.
    return EventSourceResponse(
        event_generator(),
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        },
        ping=30,  # Send keep-alive ping every 30 seconds
    )


@router.post("/cancel/{session_id}")
async def cancel_stream(session_id: str):
    """
    Cancel an ongoing chat stream.

    Args:
        session_id: Session ID to cancel (UUID from X-Session-ID header)

    Returns:
        Success status

    Raises:
        HTTPException 404: If session not found or already completed

    Example:
        POST /api/chat/cancel/550e8400-e29b-41d4-a716-446655440000

        Response 200:
        {
            "status": "cancelled"
        }
    """
    # Attempt to cancel session
    success = await stream_manager.cancel_session(session_id)

    if not success:
        raise HTTPException(
            status_code=404,
            detail="Session not found or already completed"
        )

    return {"status": "cancelled"}
