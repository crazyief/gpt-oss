#version: '3.8'

services:
  # ==========================================
  # Core Services (Required for Stage 1)
  # ==========================================

  # LLM Service - Local AI Model via llama.cpp
  # LLM Service - Local AI Model via llama.cpp
  # LLM Service - Local AI Model via llama.cpp
  llama:
    container_name: magistral-small-2506-Q8_0-llama  # Êõ¥Êñ∞ÂÆπÂô®ÂêçÁ®±‰ª•ÂèçÊò†Êñ∞Ê®°Âûã
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    command: >
      --model /models/Magistral-Small-2506_Q8_0.gguf  
      --host 0.0.0.0
      --port 8080
      --n-gpu-layers -1  
      --ctx-size 32768   
      --threads 8
      --threads-batch 8
      --batch-size 1024     
      --ubatch-size 1024  
      --flash-attn on
      --parallel 1
    ports:
      - "8080:8080"
    volumes:
      - "D:/llama_model:/models"  # ÁÑ°ÈúÄËÆäÊõ¥ÔºåÁ¢∫‰øùÊñ∞ GGUF Êñá‰ª∂Â∑≤ÊîæÈÄ≤ D:\llama_model
    environment:
      NVIDIA_VISIBLE_DEVICES: "GPU-3143337d-5132-41c1-9381-33b56ef28990"
      CUDA_VISIBLE_DEVICES: "GPU-3143337d-5132-41c1-9381-33b56ef28990"
      #CUDA_VISIBLE_DEVICES: "0"

      # ‚ö†Ô∏è ËÆäÊï∏ÂêçÁ®±ÊáâË©≤ÊòØ GGML_CUDA_FORCE_MMQÔºå‰∏çÊòØ LLAMA_CUDA_FORCE_MMQ
      #    Âè¶Â§ñÂÖà‰øùÊåÅÈ†êË®≠Ëá™ÂãïÈÅ∏ÊìáÔºå‰∏çÂº∑Âà∂ MMQÔºå‰πãÂæåÂèØ‰ª•ÂÜç A/B test
      GGML_CUDA_FORCE_MMQ: "0"  # ‰øùÊåÅÔºõËã•ÊÄßËÉΩÊÖ¢ÔºåÂèØÊ∏¨Ë©¶ "1" ‰ª•ÂÑ™Âåñ Q8_0
      GGML_CUDA_FORCE_CUBLAS: "1"

      # üöÄ FP16 + FlashAttention ÈñãÂà∞ÊúÄÂ§ß
      LLAMA_CUDA_F16: "1"
      LLAMA_CUDA_FA_ALL_QUANTS: "1"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    networks:
      - gpt-oss-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Neo4j - Knowledge Graph Database
  neo4j:
    container_name: gpt-oss-neo4j
    image: neo4j:5.13.0
    ports:
      - "7474:7474"  # HTTP interface
      - "7687:7687"  # Bolt protocol
    environment:
      - NEO4J_AUTH=neo4j/password123  # Change in production
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_apoc_import_file_use__neo4j__config=true
      - NEO4J_dbms_memory_pagecache_size=1G
      - NEO4J_dbms_memory_heap_max__size=1G
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
      - neo4j_plugins:/plugins
    networks:
      - gpt-oss-network
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "password123", "RETURN 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ChromaDB - Vector Database for Embeddings
  chroma:
    container_name: gpt-oss-chroma
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
      - ALLOW_RESET=FALSE  # Disable in production
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=["http://localhost:3000","http://localhost:8000"]
    networks:
      - gpt-oss-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Backend API - FastAPI Application
  backend:
    container_name: gpt-oss-backend
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      # LLM Configuration
      - LLM_API_URL=http://llama:8080
      - LLM_MODEL_NAME=magistral-small-2506-Q8

      # Neo4j Configuration
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=password123

      # Database Configuration (SQLite for development, PostgreSQL for production)
      - DATABASE_URL=sqlite:///./data/gpt_oss.db
      # For PostgreSQL (uncomment when ready):
      # - DATABASE_URL=postgresql://gptoss:gptoss123@postgres:5432/gptoss

      # Vector Database Configuration
      - VECTOR_DB_TYPE=chroma
      - CHROMA_HOST=chroma
      - CHROMA_PORT=8000

      # Application Settings
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - MAX_UPLOAD_SIZE=104857600  # 100MB
      - ALLOWED_EXTENSIONS=pdf,docx,xlsx,txt,md,jpg,jpeg,png

      # Security (generate new secrets in production)
      - SECRET_KEY=your-secret-key-change-in-production
      - CORS_ORIGINS=http://localhost:5173,http://127.0.0.1:5173,http://localhost:3000,http://127.0.0.1:3000
    volumes:
      - ./backend:/app
      - ./data:/app/data           # SQLite database
      - ./uploads:/app/uploads     # Uploaded documents
      - ./rag_data:/app/rag_data   # LightRAG working directory
    depends_on:
      - llama
      - neo4j
      - chroma
    networks:
      - gpt-oss-network
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ==========================================
  # Optional Services (Enable as needed)
  # ==========================================

  # PostgreSQL - Production Database (uncomment when ready)
  # postgres:
  #   container_name: gpt-oss-postgres
  #   image: postgres:16-alpine
  #   ports:
  #     - "5432:5432"
  #   environment:
  #     - POSTGRES_USER=gptoss
  #     - POSTGRES_PASSWORD=gptoss123
  #     - POSTGRES_DB=gptoss
  #   volumes:
  #     - postgres_data:/var/lib/postgresql/data
  #     - ./init.sql:/docker-entrypoint-initdb.d/init.sql
  #   networks:
  #     - gpt-oss-network
  #   healthcheck:
  #     test: ["CMD-SHELL", "pg_isready -U gptoss"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5

  # Redis - Caching and Session Management (uncomment when needed)
  # redis:
  #   container_name: gpt-oss-redis
  #   image: redis:7-alpine
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redis_data:/data
  #   networks:
  #     - gpt-oss-network
  #   command: redis-server --appendonly yes
  #   healthcheck:
  #     test: ["CMD", "redis-cli", "ping"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5

  # Frontend Development Server (for development only)
  frontend:
    container_name: gpt-oss-frontend
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    ports:
      - "5173:5173"  # Vite dev server default port
    volumes:
      - ./frontend:/app
      - /app/node_modules  # Prevent overwriting node_modules from host
    environment:
      - NODE_ENV=development
      - VITE_BACKEND_URL=http://backend:8000  # Docker service name
    depends_on:
      - backend
    networks:
      - gpt-oss-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:5173"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  gpt-oss-network:
    driver: bridge

volumes:
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  neo4j_import:
    driver: local
  neo4j_plugins:
    driver: local
  chroma_data:
    driver: local
  postgres_data:
    driver: local
  redis_data:
    driver: local