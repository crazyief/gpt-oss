#version: '3.8'

services:
  # ==========================================
  # Core Services (Required for Stage 1)
  # ==========================================

  # LLM Service - Local AI Model via llama.cpp
  llama:
    container_name: magistral-small-2506-32k  # Magistral-Small-2506-Q6_K_L @ 32k
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    command: >
      --model /models/mistralai_Magistral-Small-2506-Q6_K_L.gguf
      --host 0.0.0.0
      --port 8080
      --n-gpu-layers 99
      --ctx-size 32768
      --threads 8
      --threads-batch 8
      --batch-size 1024
      --ubatch-size 512
      --flash-attn auto
      --parallel 1
      --cache-type-k f16
      --cache-type-v f16
      --cont-batching     
    ports:
      - "8090:8080"
    volumes:
      - "D:/llama_model:/models"  # Gemma model should be in D:\llama_model
    environment:
      NVIDIA_VISIBLE_DEVICES: "GPU-3143337d-5132-41c1-9381-33b56ef28990"
      CUDA_VISIBLE_DEVICES: "GPU-3143337d-5132-41c1-9381-33b56ef28990"
      #CUDA_VISIBLE_DEVICES: "0"

      # CUDA optimizations for Q6_K quantization
      GGML_CUDA_FORCE_MMQ: "0"  # Let llama.cpp auto-select for Q6_K
      GGML_CUDA_FORCE_CUBLAS: "1"

      # FP16 + FlashAttention optimizations
      LLAMA_CUDA_F16: "1"
      LLAMA_CUDA_FA_ALL_QUANTS: "1"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    networks:
      - gpt-oss-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Neo4j - Knowledge Graph Database
  neo4j:
    container_name: gpt-oss-neo4j
    image: neo4j:5.13.0
    ports:
      - "7474:7474"  # HTTP interface
      - "7687:7687"  # Bolt protocol
    environment:
      - NEO4J_AUTH=neo4j/password123  # Change in production
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_apoc_import_file_use__neo4j__config=true
      - NEO4J_dbms_memory_pagecache_size=1G
      - NEO4J_dbms_memory_heap_max__size=1G
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
      - neo4j_plugins:/plugins
    networks:
      - gpt-oss-network
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "password123", "RETURN 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ChromaDB - Vector Database for Embeddings
  chroma:
    container_name: gpt-oss-chroma
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
      - ALLOW_RESET=FALSE  # Disable in production
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=["http://localhost:3000","http://localhost:8000"]
    networks:
      - gpt-oss-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Backend API - FastAPI Application
  backend:
    container_name: gpt-oss-backend
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      # Timezone Configuration - CRITICAL for correct timestamps
      - TZ=Asia/Shanghai  # GMT+8 timezone

      # LLM Configuration
      - LLM_API_URL=http://llama:8080
      - LLM_MODEL_NAME=falcon-h1-34b  # Updated model name

      # Neo4j Configuration
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=password123

      # Database Configuration (SQLite for development, PostgreSQL for production)
      - DATABASE_URL=sqlite:///./data/gpt_oss.db
      # For PostgreSQL (uncomment when ready):
      # - DATABASE_URL=postgresql://gptoss:gptoss123@postgres:5432/gptoss

      # Vector Database Configuration
      - VECTOR_DB_TYPE=chroma
      - CHROMA_HOST=chroma
      - CHROMA_PORT=8000

      # Application Settings
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - MAX_UPLOAD_SIZE=104857600  # 100MB
      - ALLOWED_EXTENSIONS=pdf,docx,xlsx,txt,md,jpg,jpeg,png

      # Security (generate new secrets in production)
      - SECRET_KEY=your-secret-key-change-in-production
      - CORS_ORIGINS=http://localhost:5173,http://127.0.0.1:5173,http://localhost:3000,http://127.0.0.1:3000
    volumes:
      - ./backend:/app
      - ./data:/app/data           # SQLite database
      - ./uploads:/app/uploads     # Uploaded documents
      - ./rag_data:/app/rag_data   # LightRAG working directory
    depends_on:
      - llama
      - neo4j
      - chroma
    networks:
      - gpt-oss-network
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Frontend Development Server (for development only)
  frontend:
    container_name: gpt-oss-frontend
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    ports:
      - "5173:5173"  # Vite dev server default port
    volumes:
      - ./frontend:/app
      - /app/node_modules  # Prevent overwriting node_modules from host
    environment:
      - NODE_ENV=development
      - VITE_BACKEND_URL=http://backend:8000  # Docker service name
      - TZ=Asia/Shanghai  # GMT+8 timezone for frontend
    depends_on:
      - backend
    networks:
      - gpt-oss-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:5173"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  gpt-oss-network:
    driver: bridge

volumes:
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  neo4j_import:
    driver: local
  neo4j_plugins:
    driver: local
  chroma_data:
    driver: local
  postgres_data:
    driver: local
  redis_data:
    driver: local
