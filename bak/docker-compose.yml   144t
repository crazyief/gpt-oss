#version: '3.8'

services:
  # ==========================================
  # Core Services (Required for Stage 1)
  # ==========================================

  # LLM Service - Local AI Model via llama.cpp
  llama:
    container_name: gpt-oss-llama
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    command: >
      --model /models/gpt-oss-20b-F16.gguf
      --host 0.0.0.0
      --port 8080
      --n-gpu-layers -1
      --ctx-size 4096       
      --batch-size 1024      
      --ubatch-size 2048   
      --flash-attn on
    ports:
      - "8080:8080"
    volumes:
      - "D:/llama_model:/models"
    environment:
      NVIDIA_VISIBLE_DEVICES: "GPU-3143337d-5132-41c1-9381-33b56ef28990"
      #CUDA_VISIBLE_DEVICES: "GPU-3143337d-5132-41c1-9381-33b56ef28990"
      CUDA_VISIBLE_DEVICES: "0"
      #LLAMA_CUDA_FORCE_MMQ: "true"        # Âº∑Âà∂‰ΩøÁî® customized CUDA kernel
      #LLAMA_CUDA_F16: 1              # ÂïüÁî® FP16 Ë®àÁÆó (FP16/MXFP4 ÂØ¶ÈöõÊïàËÉΩÊúÄ‰Ω≥)
      #LLAMA_CUDA_FA_ALL_QUANTS: 1   # FlashAttention ÊîØÊè¥ÂÖ®ÈÉ®ÈáèÂåñ


      # ‚ö†Ô∏è ËÆäÊï∏ÂêçÁ®±ÊáâË©≤ÊòØ GGML_CUDA_FORCE_MMQÔºå‰∏çÊòØ LLAMA_CUDA_FORCE_MMQ
      #    Âè¶Â§ñ 5090 Âª∫Ë≠∞‰∏çË¶ÅÂº∑Âà∂ MMQÔºåÊîπÁî® cuBLAS Ë∑ØÁ∑ö
      GGML_CUDA_FORCE_MMQ: "0"
      GGML_CUDA_FORCE_CUBLAS: "1"

      # üöÄ FP16 + FlashAttention ÈñãÂà∞ÊúÄÂ§ß
      LLAMA_CUDA_F16: "1"              # Áî® "1"/"0" ÊØîËºÉ‰∏ÄËá¥Á©©ÂÆö
      LLAMA_CUDA_FA_ALL_QUANTS: "1"    # FlashAttention Â•óÁî®Âà∞ÊâÄÊúâ quant Ê†ºÂºè

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    networks:
      - gpt-oss-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Neo4j - Knowledge Graph Database
  neo4j:
    container_name: gpt-oss-neo4j
    image: neo4j:5.13.0
    ports:
      - "7474:7474"  # HTTP interface
      - "7687:7687"  # Bolt protocol
    environment:
      - NEO4J_AUTH=neo4j/password123  # Change in production
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_apoc_import_file_use__neo4j__config=true
      - NEO4J_dbms_memory_pagecache_size=1G
      - NEO4J_dbms_memory_heap_max__size=1G
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
      - neo4j_plugins:/plugins
    networks:
      - gpt-oss-network
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "password123", "RETURN 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ChromaDB - Vector Database for Embeddings
  chroma:
    container_name: gpt-oss-chroma
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
      - ALLOW_RESET=FALSE  # Disable in production
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=["http://localhost:3000","http://localhost:8000"]
    networks:
      - gpt-oss-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Backend API - FastAPI Application
  backend:
    container_name: gpt-oss-backend
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      # LLM Configuration
      - LLM_API_URL=http://llama:8080
      - LLM_MODEL_NAME=gpt-oss-20b

      # Neo4j Configuration
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=password123

      # Database Configuration (SQLite for development, PostgreSQL for production)
      - DATABASE_URL=sqlite:///./data/gpt_oss.db
      # For PostgreSQL (uncomment when ready):
      # - DATABASE_URL=postgresql://gptoss:gptoss123@postgres:5432/gptoss

      # Vector Database Configuration
      - VECTOR_DB_TYPE=chroma
      - CHROMA_HOST=chroma
      - CHROMA_PORT=8000

      # Application Settings
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - MAX_UPLOAD_SIZE=104857600  # 100MB
      - ALLOWED_EXTENSIONS=pdf,docx,xlsx,txt,md,jpg,jpeg,png

      # Security (generate new secrets in production)
      - SECRET_KEY=your-secret-key-change-in-production
      - CORS_ORIGINS=["http://localhost:3000","http://localhost:5173"]
    volumes:
      - ./backend:/app
      - ./data:/app/data           # SQLite database
      - ./uploads:/app/uploads     # Uploaded documents
      - ./rag_data:/app/rag_data   # LightRAG working directory
    depends_on:
      - llama
      - neo4j
      - chroma
    networks:
      - gpt-oss-network
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ==========================================
  # Optional Services (Enable as needed)
  # ==========================================

  # PostgreSQL - Production Database (uncomment when ready)
  # postgres:
  #   container_name: gpt-oss-postgres
  #   image: postgres:16-alpine
  #   ports:
  #     - "5432:5432"
  #   environment:
  #     - POSTGRES_USER=gptoss
  #     - POSTGRES_PASSWORD=gptoss123
  #     - POSTGRES_DB=gptoss
  #   volumes:
  #     - postgres_data:/var/lib/postgresql/data
  #     - ./init.sql:/docker-entrypoint-initdb.d/init.sql
  #   networks:
  #     - gpt-oss-network
  #   healthcheck:
  #     test: ["CMD-SHELL", "pg_isready -U gptoss"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5

  # Redis - Caching and Session Management (uncomment when needed)
  # redis:
  #   container_name: gpt-oss-redis
  #   image: redis:7-alpine
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redis_data:/data
  #   networks:
  #     - gpt-oss-network
  #   command: redis-server --appendonly yes
  #   healthcheck:
  #     test: ["CMD", "redis-cli", "ping"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5

  # Frontend Development Server (for development only)
  # frontend:
  #   container_name: gpt-oss-frontend
  #   build:
  #     context: ./frontend
  #     dockerfile: Dockerfile.dev
  #   ports:
  #     - "3000:3000"
  #   volumes:
  #     - ./frontend:/app
  #     - /app/node_modules
  #   environment:
  #     - NODE_ENV=development
  #     - VITE_API_URL=http://localhost:8000
  #   networks:
  #     - gpt-oss-network
  #   command: npm run dev

networks:
  gpt-oss-network:
    driver: bridge

volumes:
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  neo4j_import:
    driver: local
  neo4j_plugins:
    driver: local
  chroma_data:
    driver: local
  postgres_data:
    driver: local
  redis_data:
    driver: local